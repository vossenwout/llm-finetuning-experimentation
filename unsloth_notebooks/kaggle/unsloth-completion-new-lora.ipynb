{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!mamba install --force-reinstall aiohttp -y\n!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-08T13:02:58.416616Z","iopub.execute_input":"2024-06-08T13:02:58.417021Z","iopub.status.idle":"2024-06-08T13:07:53.308098Z","shell.execute_reply.started":"2024-06-08T13:02:58.416993Z","shell.execute_reply":"2024-06-08T13:07:53.306897Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Load base model","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 4000 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nbase_llama_3 = \"unsloth/llama-3-8b-bnb-4bit\"\nfinetuned = \"pookie3000/pg_lora_completion_run2\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = base_llama_3, # \"unsloth/mistral-7b\" for 16bit loading\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:15:51.975860Z","iopub.execute_input":"2024-06-08T13:15:51.976720Z","iopub.status.idle":"2024-06-08T13:16:50.755042Z","shell.execute_reply.started":"2024-06-08T13:15:51.976677Z","shell.execute_reply":"2024-06-08T13:16:50.753955Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-06-08 13:16:03.219850: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-08 13:16:03.219952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-08 13:16:03.387251: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0bce087bd844d89aa2a97f6621fd12"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.5\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927a8949f000491bad84cd1148d89fc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e10c70544db4d32be4e8d0c05c67ecb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4f2d5b6326448da94d6604f23ad31b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5b37c8029c4e1fb38341baf7a887a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ad15e225d7941eebb31ede6e4f86e9c"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    max_seq_length = max_seq_length,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:18:49.030903Z","iopub.execute_input":"2024-06-08T13:18:49.032207Z","iopub.status.idle":"2024-06-08T13:18:52.579687Z","shell.execute_reply.started":"2024-06-08T13:18:49.032174Z","shell.execute_reply":"2024-06-08T13:18:52.578854Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"pookie3000/pg_essays_split_1000_t\", split = \"train\")\nEOS_TOKEN = tokenizer.eos_token\nBOS_TOKEN = tokenizer.bos_token\n\ndef formatting_func(example):\n    return example[\"text\"] + EOS_TOKEN\n#def formatting_func(example):\n#    return BOS_TOKEN + example[\"text\"] + EOS_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:19:10.677758Z","iopub.execute_input":"2024-06-08T13:19:10.678160Z","iopub.status.idle":"2024-06-08T13:19:12.455143Z","shell.execute_reply.started":"2024-06-08T13:19:10.678126Z","shell.execute_reply":"2024-06-08T13:19:12.454145Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.58M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"010d2e8f0d7846ebb04993e16e67eb43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9190f7357740299e3c8a04c2670353"}},"metadata":{}}]},{"cell_type":"code","source":"for item in dataset['text'][:3]:\n  print(\"##########\")\n  print(item[:500])\n  print(\"##########\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T12:59:03.919972Z","iopub.status.idle":"2024-06-02T12:59:03.920454Z","shell.execute_reply.started":"2024-06-02T12:59:03.920205Z","shell.execute_reply":"2024-06-02T12:59:03.920240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    tokenizer = tokenizer,\n    max_seq_length = max_seq_length,\n    packing = False, # Packs short sequences together to save time!\n    formatting_func = formatting_func,\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        #warmup_ratio = 0.05,\n        #max_grad_norm = 1.0,\n        num_train_epochs = 2,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.1,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T14:33:40.253924Z","iopub.execute_input":"2024-06-08T14:33:40.255041Z","iopub.status.idle":"2024-06-08T14:33:41.366944Z","shell.execute_reply.started":"2024-06-08T14:33:40.254990Z","shell.execute_reply":"2024-06-08T14:33:41.366077Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1020 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f3f7e0a92f496e90b928ab59f940ee"}},"metadata":{}}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T14:33:43.705237Z","iopub.execute_input":"2024-06-08T14:33:43.705885Z","iopub.status.idle":"2024-06-08T15:45:37.592789Z","shell.execute_reply.started":"2024-06-08T14:33:43.705852Z","shell.execute_reply":"2024-06-08T15:45:37.591924Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 1,020 | Num Epochs = 2\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 254\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [254/254 1:11:33, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.112200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.180700</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.100400</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.138800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.099300</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.122400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.238700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.229600</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.132300</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.227400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.221500</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.193300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.207000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.189300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.255000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.233300</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.265700</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.188700</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.245800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.354700</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.234900</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.226500</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.222400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.247200</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.296000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.185100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.413500</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.191000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.248300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.325700</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.302600</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.355800</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.270400</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.296300</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.236900</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.260000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.240500</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.360500</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.219300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.317100</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.241500</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.282800</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.309700</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.382100</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.311600</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.243100</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.374500</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.304000</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.262000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.257900</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.336500</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.279300</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.240400</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.186400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.324100</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.229700</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.276400</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.267100</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.392800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.250700</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.276800</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.301600</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.253200</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.173500</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.230400</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.216800</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.286200</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.277200</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.268800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.445500</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.303500</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.316000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.314800</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.314000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.261000</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.410000</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.309400</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.403100</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.337300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.415100</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.417900</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.242900</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.321600</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.342300</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.299400</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.368000</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.358700</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.363400</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.309900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.357400</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.260500</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.389100</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.302400</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.384900</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.243600</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.328100</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.238400</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.298500</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.221300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.369200</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>0.490100</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>0.460000</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>0.226600</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.291000</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.444500</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>0.349700</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.273900</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>0.257300</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>0.388100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.342500</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>0.277900</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>0.270100</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>0.292100</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>0.325300</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.302700</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>0.330900</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>0.279200</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>0.417600</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.336300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.363900</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>0.288600</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>0.383100</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>0.294200</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>0.236600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.272700</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>0.324600</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>0.326600</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>0.277700</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.126900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.142400</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.103300</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>0.130200</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>0.097500</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.103400</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.108600</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.128000</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>0.108700</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.125800</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.106400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.092500</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.111200</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>0.103600</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>0.108300</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>0.092600</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.078200</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>0.091400</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.101200</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.085400</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.076800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.119100</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>0.115200</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>0.112500</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>0.125500</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>0.112200</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.087900</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.104000</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>0.091300</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>0.134600</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>0.108300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.117500</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>0.089800</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>0.110000</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>0.068600</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>0.081100</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.110900</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>0.139000</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>0.113500</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>0.082400</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>0.127200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.091000</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>0.105900</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>0.126900</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>0.102300</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>0.097800</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.115500</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>0.081600</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>0.113400</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>0.104600</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0.132400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.087500</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>0.100600</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.117300</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>0.253700</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>0.100500</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.089500</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>0.110900</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>0.127900</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>0.068800</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>0.116800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.094600</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>0.094600</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>0.127500</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>0.144500</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>0.112400</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.143900</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>0.100800</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>0.117500</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>0.151000</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>0.099300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.126400</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>0.127500</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>0.136800</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>0.123700</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>0.128700</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.121800</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>0.118400</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>0.098300</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>0.157000</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>0.144600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.132900</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>0.164300</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>0.098900</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>0.123600</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>0.120100</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.126900</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>0.086300</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>0.118100</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>0.161800</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.125600</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>0.162400</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>0.120700</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>0.135500</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>0.140100</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.183200</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>0.142400</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>0.139200</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>0.184900</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>0.126700</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.155200</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>0.144700</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>0.202000</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>0.142900</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>0.188900</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.182100</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>0.136400</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>0.183800</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>0.246100</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>0.174500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.199800</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>0.228300</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>0.228600</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>0.218800</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>0.255900</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.222700</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>0.230300</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>0.258200</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>0.262900</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>0.261800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.222100</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>0.209900</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>0.248200</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>0.233200</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>0.202300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Save lora","metadata":{}},{"cell_type":"code","source":"model.push_to_hub_merged(\"pookie3000/pg_lora_completion_run6\", tokenizer, save_method = \"lora\", token = \"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\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:45:51.563816Z","iopub.execute_input":"2024-06-08T15:45:51.564140Z","iopub.status.idle":"2024-06-08T15:45:57.273961Z","shell.execute_reply.started":"2024-06-08T15:45:51.564114Z","shell.execute_reply":"2024-06-08T15:45:57.272903Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Unsloth: Saving LoRA adapters. Please wait...\nSaved lora model to https://huggingface.co/pookie3000/pg_lora_completion_run6\n","output_type":"stream"}]}]}