{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-16T15:19:22.766720Z","iopub.status.busy":"2024-06-16T15:19:22.766013Z","iopub.status.idle":"2024-06-16T15:23:57.922413Z","shell.execute_reply":"2024-06-16T15:23:57.921259Z","shell.execute_reply.started":"2024-06-16T15:19:22.766685Z"},"trusted":true},"outputs":[],"source":["%%capture\n","!mamba install --force-reinstall aiohttp -y\n","!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n","!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n","\n","# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n","!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"markdown","metadata":{},"source":["## Load base model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T15:24:18.236090Z","iopub.status.busy":"2024-06-16T15:24:18.235713Z","iopub.status.idle":"2024-06-16T15:25:10.928037Z","shell.execute_reply":"2024-06-16T15:25:10.927057Z","shell.execute_reply.started":"2024-06-16T15:24:18.236056Z"},"trusted":true},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 8000 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","base_llama_3 = \"pookie3000/llama-3-8b-bnb-4bit-for-chat-training\"\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = base_llama_3,\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### (If you add new special tokens, you also need to train embedding I think.... (experimental))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T15:25:12.101775Z","iopub.status.busy":"2024-06-16T15:25:12.100799Z","iopub.status.idle":"2024-06-16T15:25:25.244257Z","shell.execute_reply":"2024-06-16T15:25:25.243078Z","shell.execute_reply.started":"2024-06-16T15:25:12.101741Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unsloth: Offloading input_embeddings to disk to save VRAM\n","Unsloth: Offloading output_embeddings to disk to save VRAM\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]},{"name":"stdout","output_type":"stream","text":["Unsloth: Casting embed_tokens to float32\n","Unsloth: Casting lm_head to float32\n"]}],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    max_seq_length = max_seq_length,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### (If you do not add special tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    max_seq_length = max_seq_length,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Load dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T15:25:56.241414Z","iopub.status.busy":"2024-06-16T15:25:56.241044Z","iopub.status.idle":"2024-06-16T15:25:58.478310Z","shell.execute_reply":"2024-06-16T15:25:58.477365Z","shell.execute_reply.started":"2024-06-16T15:25:56.241385Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36db1f2270a840faa241d7475f0ed47d","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a58d59ad68144191a693b2649943303f","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/316k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2eeddbb8e7f458b93982ff828aeca88","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5d12e8b0f454a11a5948e12cddd1344","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","def formatting_prompts_func(examples):\n","    convos = examples[\"conversations\"]\n","    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n","    return { \"text\" : texts, }\n","pass\n","\n","hf_dataset_path = \"pookie3000/trump-interviews\"\n","dataset = load_dataset(hf_dataset_path, split=\"train\")\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T15:26:01.469552Z","iopub.status.busy":"2024-06-16T15:26:01.468658Z","iopub.status.idle":"2024-06-16T15:26:01.475708Z","shell.execute_reply":"2024-06-16T15:26:01.474761Z","shell.execute_reply.started":"2024-06-16T15:26:01.469516Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'role': 'user', 'content': 'But he doesn’t have the authority to do that as you know.'}, {'role': 'assistant', 'content': 'What? Okay, are you ready? Let’s have this now, just now, because it’s interest – let’s keep it interesting, right? I like Mike Pence very much. He’s a very fine man. He’s a very nice man. He made a mistake. His lawyer said, you cannot move. I call them the human conveyer belt. I said, even if the votes you mean – I talked to his lawyer – even if the votes are absolutely fraudulent, he can’t send – yes, sir, he can’t send them back. And the Democrats played it and the RINOs played it. And then, the election was over. They told them he couldn’t do it. And Mike said to me, I can’t do it. The lawyers told me I can’t do it, they can’t do it. But the lawyers were wrong because right after the election, they all met, the RINOs and the Democrats, and they worked out a plan to make sure that future vice presidents don’t do what I said you could do.'}, {'role': 'user', 'content': 'That’s not what happened. You’re referring to the Electoral Count Act.'}, {'role': 'assistant', 'content': 'That’s exactly – no, excuse me – Kaitlan, why did they change the law then saying that you can’t do it?'}]\n","----------------------------------------------\n","<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","But he doesn’t have the authority to do that as you know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","What? Okay, are you ready? Let’s have this now, just now, because it’s interest – let’s keep it interesting, right? I like Mike Pence very much. He’s a very fine man. He’s a very nice man. He made a mistake. His lawyer said, you cannot move. I call them the human conveyer belt. I said, even if the votes you mean – I talked to his lawyer – even if the votes are absolutely fraudulent, he can’t send – yes, sir, he can’t send them back. And the Democrats played it and the RINOs played it. And then, the election was over. They told them he couldn’t do it. And Mike said to me, I can’t do it. The lawyers told me I can’t do it, they can’t do it. But the lawyers were wrong because right after the election, they all met, the RINOs and the Democrats, and they worked out a plan to make sure that future vice presidents don’t do what I said you could do.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","That’s not what happened. You’re referring to the Electoral Count Act.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","That’s exactly – no, excuse me – Kaitlan, why did they change the law then saying that you can’t do it?<|eot_id|>\n"]}],"source":["print(dataset[20][\"conversations\"])\n","print(\"----------------------------------------------\")\n","print(dataset[20][\"text\"])"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"markdown","metadata":{},"source":["### Normal"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        num_train_epochs = 2,\n","        #max_steps = 60,\n","        learning_rate = 2e-4,\n","        fp16 = not torch.cuda.is_bf16_supported(),\n","        bf16 = torch.cuda.is_bf16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\",\n","    ),\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### If you train new embeddings (you need to lower batch size and gradient accum steps)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T15:28:31.595924Z","iopub.status.busy":"2024-06-16T15:28:31.595535Z","iopub.status.idle":"2024-06-16T15:28:31.863821Z","shell.execute_reply":"2024-06-16T15:28:31.862881Z","shell.execute_reply.started":"2024-06-16T15:28:31.595890Z"},"trusted":true},"outputs":[],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 1,\n","        gradient_accumulation_steps = 1,\n","        warmup_steps = 5,\n","        num_train_epochs = 5,\n","        #max_steps = 60,\n","        learning_rate = 2e-4,\n","        fp16 = not torch.cuda.is_bf16_supported(),\n","        bf16 = torch.cuda.is_bf16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\",\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T15:28:36.170026Z","iopub.status.busy":"2024-06-16T15:28:36.168924Z","iopub.status.idle":"2024-06-16T16:06:50.195555Z","shell.execute_reply":"2024-06-16T16:06:50.193923Z","shell.execute_reply.started":"2024-06-16T15:28:36.169981Z"},"trusted":true},"outputs":[],"source":["trainer_stats = trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T16:39:55.027498Z","iopub.status.busy":"2024-06-16T16:39:55.027131Z","iopub.status.idle":"2024-06-16T16:40:00.858775Z","shell.execute_reply":"2024-06-16T16:40:00.858042Z","shell.execute_reply.started":"2024-06-16T16:39:55.027471Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","Is Joe Biden competent as president?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","I think so. I think so. He’s a very man of God. He’s a very good man. I think he’s a very good president. I’d like to see the country do better. And we have the same thoughts in that. We want to see the country do better. We want to see the country rocking and rolling. And we can do it. We can do it.<|eot_id|>\n"]}],"source":["FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Is Joe Biden competent as president?\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2000, use_cache = True)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T16:11:52.891073Z","iopub.status.busy":"2024-06-16T16:11:52.890147Z","iopub.status.idle":"2024-06-16T16:13:44.602424Z","shell.execute_reply":"2024-06-16T16:13:44.601279Z","shell.execute_reply.started":"2024-06-16T16:11:52.891041Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unsloth: Saving LoRA adapters. Please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3230e5d78f8448eab11e693808eb916f","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19151b5ca5204052bb67fadea1f81dc5","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saved lora model to https://huggingface.co/pookie3000/trump_lora\n"]}],"source":["hf_token = \"YOUR_HF_TOKEN\"\n","model.push_to_hub_merged(\"pookie3000/trump_lora\", tokenizer, save_method = \"lora\", token = hf_token)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
