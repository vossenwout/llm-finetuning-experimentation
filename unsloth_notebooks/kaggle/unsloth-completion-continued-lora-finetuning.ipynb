{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!mamba install --force-reinstall aiohttp -y\n!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load base model","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 4000 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nbase_llama_3 = \"unsloth/llama-3-8b-bnb-4bit\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = base_llama_3,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    max_seq_length = max_seq_length,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load lora adapter","metadata":{}},{"cell_type":"code","source":"model.delete_adapter(\"default\")\n\nhf_path = \"pookie3000/pg_lora_completion_run2\"\nmodel.load_adapter(hf_path, \"default\");\nmodel.set_adapter(\"default\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"pookie3000/pg_essays_split_1000_t\", split = \"train\")\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_func(example):\n    return example[\"text\"] + EOS_TOKEN","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for item in dataset['text'][:3]:\n  print(\"##########\")\n  print(item[:500])\n  print(\"##########\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    tokenizer = tokenizer,\n    max_seq_length = max_seq_length,\n    packing = False, # Packs short sequences together to save time!\n    formatting_func = formatting_func,\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        #warmup_ratio = 0.05,\n        #max_grad_norm = 1.0,\n        num_train_epochs = 2,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.1,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save","metadata":{}},{"cell_type":"code","source":"model.push_to_hub_merged(\"pookie3000/pg_lora_completion_run6\", tokenizer, save_method = \"lora\", token = \"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\")","metadata":{},"execution_count":null,"outputs":[]}]}